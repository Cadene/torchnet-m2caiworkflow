{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "videos = []\n",
    "for i in range(1,28):\n",
    "    steps = map(lambda x: x.strip().split(\"\\t\")[1], open(\"../annotations/workflow_video_%02d.txt\" % i, \"r\").readlines()[1:])\n",
    "    videos.append(steps[24::25]) # keep 1 frame out of 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TrocarPlacement', 'Preparation', 'CalotTriangleDissection', 'ClippingCutting', 'GallbladderDissection', 'GallbladderPackaging', 'CleaningCoagulation', 'GallbladderRetraction']\n"
     ]
    }
   ],
   "source": [
    "unique_steps = list(reduce(lambda s1, s2: s1.union(s2), map(lambda s: set(s), videos)))\n",
    "\n",
    "# Manual writting of steps to have meaningful order\n",
    "ordered_unique_steps = \\\n",
    "    [\"TrocarPlacement\", \"Preparation\", \"CalotTriangleDissection\", \"ClippingCutting\", \"GallbladderDissection\",\n",
    "     \"GallbladderPackaging\", \"CleaningCoagulation\", \"GallbladderRetraction\"]\n",
    "p = len(ordered_unique_steps)\n",
    "\n",
    "if len(set(ordered_unique_steps).difference(set(unique_steps))) > 0:\n",
    "    raise Exception(\"Sets do not match\")\n",
    "else:\n",
    "    print ordered_unique_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This map will be used to get the index of a string step\n",
    "step_index = {v: k for k, v in zip(range(p), ordered_unique_steps)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of elements sanity check\n",
    "\n",
    "Let's check if the number of annotations extracted from annotations file match the number of images in the folder. Seems like they don't, there seem to be 1 or 2 frames more than annotations. Whatever, let's ignore the additional images and simply use the annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "videos_annotations_len = []\n",
    "for steps in videos:\n",
    "    videos_annotations_len.append(len(steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-cf3bc1e1aa70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvideos_images_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".jpg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../files.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvideos_images_len\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "videos_images_len = []\n",
    "files = map(lambda x: x.strip().replace(\".jpg\", \"\").split(\"-\"), open(\"../files.txt\", \"r\").readlines())\n",
    "groups = pd.DataFrame(files).groupby(0)\n",
    "for name, group in groups:\n",
    "    videos_images_len.append(len(group[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print zip(videos_annotations_len, videos_images_len)\n",
    "print np.array(videos_annotations_len) - np.array(videos_images_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data split\n",
    "\n",
    "Let's compute a train / val split and write the list of images / annotations in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "splits = [\n",
    "    [1,2,3],\n",
    "    [4,5,6,7],\n",
    "    [8,9,10,11,12],\n",
    "    [13,14,15],\n",
    "    [16,17,18],\n",
    "    [19,20,21,22],\n",
    "    [23,24,25,26,27]\n",
    "]\n",
    "\n",
    "splits = map(lambda x: map(lambda y: y-1, x), splits) # -1 to video ids to have good indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for j, test_inds in enumerate(splits):\n",
    "    out_test = \"\"\n",
    "\n",
    "    for test_ind in test_inds:\n",
    "        for i, step in enumerate(videos[test_ind]):\n",
    "            out_test += \"workflow_video_%02d-%04d.jpg, %d\\n\" % (test_ind + 1, i + 1, step_index[step])\n",
    "        \n",
    "    open(\"../dataset2/valset_fold_%d.txt\" % j, \"w\").write(out_test)\n",
    "    \n",
    "    train_inds = list(set(range(27)).difference(test_inds))\n",
    "    \n",
    "    out_train = \"\"\n",
    "\n",
    "    for train_ind in train_inds:\n",
    "        for i, step in enumerate(videos[train_ind]):\n",
    "            out_train += \"workflow_video_%02d-%04d.jpg, %d\\n\" % (train_ind + 1, i + 1, step_index[step])\n",
    "\n",
    "    open(\"../dataset2/trainset_fold_%d.txt\" % j, \"w\").write(out_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
