\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{4321}
\begin{document}

%%%%%%%%% TITLE
\title{M2CAI Workflow Challenge : Convolutional Neural Networks with Time Smoothing and Hidden Markov Model for Video Frames Classification}

\author{Remi Cadene \hspace{1cm} Thomas Robert \hspace{1cm} Nicolas Thome \hspace{1cm} Matthieu Cord\\
Sorbonne Universites, UPMC Univ Paris 06, CNRS, LIP6 UMR 7606, 4 place Jussieu, 75005 Paris\\
{\tt\small \{remi.cadene, thomas.robert, nicolas.thome, matthieu.cord@lip6.fr\}}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
  This report present our work on the M2CAI workflow challenge consisting in recognizing
at which step of an operation each frame of a video belong. Firstly we fine tune a recent
deep convolutional neural network pretrained on ImageNet. Secondly, we apply temporal smoothing
using simple temporal averaging of the predictions and a hidden Markov model. 
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

The \emph{M2CAI workflow} challenge consists in analyzing endoscopic videos of
minimally invasive surgical operations of cholecystectomy. The task at hand is
to detect at which of the 8 phases of the operation each frame belong. This can
be useful to evaluate a surgeon or to trigger automatic actions in the operating
room for example.

For this challenge, the task must be carried out ``online'', meaning the prediction
for a given frame can only be made based on past frames and predictions, without
any knowledge of the future. This simulates a process of prediction in real-time.

The dataset consists of 27 videos in the training set, ranging from 14 to 66
minutes and a test set of 15 videos. The videos have a resolution of 1920 $\times$
1080 pixels and are shot at 25 frames per second at the IRCAD research center in
Strasbourg, France.

%-------------------------------------------------------------------------
\subsection{Our approach}

The approach described in this paper consists of two main step, one step of visual
recognition without any temporal component using a deep Convolutional Neural
Network (CNN), and one step of temporal smoothing to improve the coherence of the
predicted sequence using averaging and a Hidden Markov Model (HMM).

\section{Visual recognition: Deep CNN}

The first step of our approach is to learn a classification model of video frames.

To do so, we split randomly the training data into two sets of videos, a full training set of 22 videos and a validation
set of 5 videos. In our approach, the training set is made of the following video : 1,3,4,5,6,7,8,11,12,14,15,16,17,18,19,20,21,22,23,24,25,26. The testing set is made of : 2,9,10,13,27.

Secondly, we extract one frame every 25 frames, returning 1 frame per second
of the video. This is done both on the full training set and testing set (the one without any label).

Thirdly, we train our frame classifier and validate it on our validation set. In this study, we compare several approaches detailed in the following subsections.

\subsection{Pretrain CNN as Features Extractor with SVM}

This approach consists of using a pretrain CNN to vectorize the training and validation sets and to train a multi-class classifier on the features vectors, such as a linear model with a cross entropy criterion or Support Vector Machines with a one versus all strategy.
Usually the features are extracted at the end of the CNN after a fully connected layer or an activation functions. This is the usual baseline method for transfer learning %CITE VGG.

In this challenge, we use features extracted from the penultimate layer of InceptionV3 \cite{szegedy2015rethinking} (2048 dimensions) and train a linear model with a cross entropy criterion. Our preprocessing is as follow. As InceptionV3 takes as input images of size 299x299. We randomly rescale the original images between 299 and 330 pixels of width or height based on the smallest fitting dimension. Then, we randomly crop a square region of size 299x299. Finally, we normalize each pixels using the mean and standard deviation processed on ImageNet.

%https://github.com/Cadene/torchnet-m2caiworkflow/blob/master/src/inceptionv3.lua

\subsection{Fine Tuning Pretrain CNN}

Fine tuning consists of training a pretrain CNN on a smaller dataset. Typically, the last fully connected layers, which can be viewed as classification layers, are reset and a smaller learning rate is applied to the pretrain layers. By doing so, the goal is to adapt the representations learned to the new dataset. The more different is the latter from the original dataset, the more layers must be reset.

In this study, we remove the last layer of a pretrain InceptionV3 on ImageNet and add a new fully connected of output size equal to 4. We use Adam \cite{Kingma14} which does not need any smaller learning rate for pretrain layers. We tune using grid search the learning rate et the learning rate decay (which decrease the adaptive learning rate of each parameters by a factor after each mini batch).

We also fine tune an other pretrain CNN on ImageNet called Residual Network-200. As it takes as input images of size 221x221, we use the same preprocessing with a minimal size and a crop size of 221 and a maximum size of 240.


\subsection{Other Baselines}

The first baseline is to train an InceptionV3 From Scratch. It means that we reset all its parameters with LSUV method %CITE.

The second baseline is to Fine Tune an Inception-V3 with the WELDON aggregation layer plugged at the end. %Define weldon
We use the same preprocessing with a minimal size and crop size of 448 and a maximum size of 463.


\section{Temporal smoothing: Averaging and HMM}

\subsection{Averaging}

Using a model defined in the previous section, we obtain for each images a vector of log-probabilities.
To temporally smooth the predictions, we average the vectors across the
last 15 frames (corresponding to 15 seconds of the video). This means that our
smoothed prediction has a lag of 7.5 seconds. However, the metric of the challenge
allows a 10-second-margin in the predictions, meaning that it is not considered
problematic to have a slight lag in our prediction. We therefore take advantage
of this tolerance to improve the smoothness of the predictions.

\subsection{Hidden Markov Model}

In addition to the averaging, we propose to use an HMM to model the transition
between the various steps of the operation. To do so, we consider that the step of the operation is the discrete hidden state $x_t$,
from which we only observe a noisy vector $y_t$ that is our averaged vector of log-probabilities from
the network.

\paragraph{Training} A HMM has 3 kind of parameters: the initial state probabilities, the matrix of probabilities
of transition between states from one time-step to the next, and the parameters
regarding emission of observations for each possible step.

We compute those information on the training set. The initial state probabilities and the matrix of transition are computed by simple counting. We chose to model the emission of
observation with a gaussian distribution. To do so, we computer for each step the average observation and its covariance matrix.

\paragraph{Offline prediction} In this study, the transition matrix of our trained HMM is parse, which impose a lot of structure on the predicted sequence of states. By applying Viterbi algorithm, we ``decode'' a sequence of observations to obtain the most likely sequence of states. This is done for our offline prediction.

\paragraph{Online prediction} However, for this challenge, the predictions must
be given in online mode. Therefore, for our online predictions, to predict the state
$x_t$ we apply the Viterbi algorithm on the sequence $y_1,...,y_t$ and keep the
last state of the predicted sequence. This process ensure that we are working in online
mode. However, it decreases the performance of the HMM, because the constrains on the transition
matrix are no longer enforced on the predicted sequence.

\subsection{Post-processing to produce requested files}

As for now, our two-step model gives us predictions at a rate of 1 frame per second. To come
back to the required rate of 25 fps, we simply copy each prediction 25 times, and
then crop or pad the predicted sequence to match exactly the number of frames in
the video.

\section{Experiments}


\begin{table}
	\begin{center}
		\begin{tabular}{|l|c|}
			\hline
			Classification Model & Validation Accuracy Top 1 (\%) \\
			\hline\hline
			InceptionV3 Extraction &  \\
			InceptionV3 Fine Tuned &  \\
			ResNet200 Fine Tuned & \\
			InceptionV3 From Scratch & \\
			IncpetionV3 Weldon & \\
			\hline
		\end{tabular}
	\end{center}
	\caption{}
\end{table}


\begin{table}
	\begin{center}
		\begin{tabular}{|l|c|}
			\hline
			Temporal Model & Score (\%) \\
			\hline\hline
			Baseline &  \\
			Offline &  \\
			Online & \\
			\hline
		\end{tabular}
	\end{center}
	\caption{}
\end{table}


VIsualisation



\section{Conclusion}

In this challenge, we tried several approaches. We validated that fine tuning Convolutional Neural Networks is a good approach on this kind of datasets.






{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
