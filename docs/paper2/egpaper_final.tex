\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{4321}
\begin{document}

%%%%%%%%% TITLE
\title{M2CAI Workflow Challenge : Convolutional Neural Networks with Time Smoothing and Hidden Markov Model for Video Frames Classification}

\author{Remi Cadene \hspace{1cm} Thomas Robert \hspace{1cm} Nicolas Thome \hspace{1cm} Matthieu Cord\\
Sorbonne Universites, UPMC Univ Paris 06, CNRS, LIP6 UMR 7606, 4 place Jussieu, 75005 Paris\\
{\tt\small \{remi.cadene, thomas.robert, nicolas.thome, matthieu.cord@lip6.fr\}}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
  This report present our work on the M2CAI workflow challenge consisting in recognizing
at which step of an operation each frame of a video belong. Firstly we fine tune a recent
deep convolutional neural network pretrained on ImageNet. Secondly, we apply temporal smoothing
using simple temporal averaging of the predictions and a hidden Markov model. 
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

The \emph{M2CAI workflow} challenge consists in analyzing endoscopic videos of
minimally invasive surgical operations of cholecystectomy. The task at hand is
to detect at which of the 8 phases of the operation each frame belong. This can
be useful to evaluate a surgeon or to trigger automatic actions in the operating
room for example.

For this challenge, the task must be carried out ``online'', meaning the prediction
for a given frame can only be made based on past frames and predictions, without
any knowledge of the future. This simulates a process of prediction in real-time.

The dataset consists of 27 videos in the training set, ranging from 14 to 66
minutes and a test set of 15 videos. The videos have a resolution of 1920 $\times$
1080 pixels and are shot at 25 frames per second at the IRCAD research center in
Strasbourg, France.

%-------------------------------------------------------------------------
\subsection{Our approach}

The approach described in this paper consists of two main step, one step of visual
recognition without any temporal component using a deep Convolutional Neural
Network (CNN), and one step of temporal smoothing to improve the coherence of the
predicted sequence using averaging and a Hidden Markov Model (HMM).

\section{Visual recognition: Deep CNN}

The first step of our approach is to learn a classification model of video frames.

To do so, we split randomly the training data into two sets of videos, a full training set of 22 videos and a validation
set of 5 videos. In our approach, the training set is made of the following video : 1,3,4,5,6,7,8,11,12,14,15,16,17,18,19,20,21,22,23,24,25,26. The testing set is made of : 2,9,10,13,27.

Secondly, we extract one frame every 25 frames, returning 1 frame per second
of the video. This is done both on the full training set and testing set (the one without any label).

Thirdly, we train our frame classifier and validate it on our validation set. In this study, we compare several approaches detailed in the following subsections.

\subsection{Pretrain CNN as Features Extractor with SVM}

This approach consists of using a pretrain CNN to vectorize the training and validation sets and to train a linear model such as a Support Vector Machine on the features vectors.
Usually the features are extracted at the end of the CNN after a fully connected layer,

 This is the usual basline method for transfer learning %CITE VGG

Inception-V3 as features extractor with SVM.

\subsection{Fine Tuning Pretrain CNN}

Fine tuned Inception-V3}

Fine tuned Residual Network-200

\subsection{Other Baselines}

Training Large CNN From Scratch}

Fine Tuning Inception-V3 with WELDON aggregation



.an Inception-V3 \cite{szegedy2015rethinking} model and validating the
different hyperparamaters for the optimization. We
keep the weights at the epoch that gave the best results on the validation set.

Note that the network is initialized with the weights learnt for the ILSVRC challenge
(classification of 1.2 million images across 1000 categories). It is fine tuned
for this task with Adam \cite{Kingma14} optimization algorithm.



\section{Temporal smoothing: averaging}

\subsection{Averaging}

From the previous step of our method, we obtain for each image in the test set
a vector of log-probabilities, with for each possible step the number of networks predicting
that the frame belong to this step of the operation.

To temporally smooth the predictions, we average the vectors of log-probabilities across the
last 15 frames (corresponding to 15 seconds of the video). This means that our
smoothed prediction has a lag of 7.5 seconds. However, the metric of the challenge
allows a 10-second-margin in the predictions, meaning that it is not considered
problematic to have a slight lag in our prediction. We therefore take advantage
of this tolerance to improve the smoothness of the predictions.

\subsection{Hidden Markov Model}

In addition to the averaging, we propose to use an HMM to model the transition
between the various steps of the operation.

To do so, we consider that the step of the operation is the discrete hidden state $x_t$,
from which we only observe a noisy vector $y_t$ that is our averaged vector of log-probabilities from
the network.

\paragraph{Training} An HMM has 3 parameters: the initial state probabilities, the matrix of probabilities
of transition between states from one timestep to the next, and the parameters
regarding the emission of the observations for each possible step.

Those information are directly accessible in the trainset and computed on it.
Probabilities are computed by simple counting. We chose to model the emission of
observation with a gaussian distribution so for each step we compute the average
observation and its covariance matrix.

\paragraph{Offline prediction} An HMM is especially useful in offline mode, especially
in our case where the transition matrix is sparse imposing a lot of structure on
the predicted sequence of states. By applying Viterbi algorithm, we can ``decode''
a sequence of observations (votes from the networks) to obtain the most likely
sequence of states. This is done for our offline prediction.

\paragraph{Online prediction} However, for this challenge, the predictions must
be given in online mode. Therefore, for our online predictions, to predict the state
$x_t$ we apply the Viterbi algorithm on the sequence $y_1,...,y_t$ and keep the
last state of the predicted sequence. This process ensure we are working in only
mode but decrease the performance of the HMM, because the constrains on the transition
matrix are no longer really enforced on the predicted sequence.

\section{Post-processing to produce requested files}

Our previous step gives us prediction at a rate of 1 frame per second. To come
back to the required rate of 25 fps, we simply copy each prediction 25 times, and
then crop or pad the predicted sequence to match exactly the number of frames in
the video.












{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
